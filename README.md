# clip_guided_diffusion

This is codebase from [openai/CLIP](https://github.com/openai/CLIP). <br/> 
CLIP (Contrastive Language-Image Pre-Training) is a neural network trained on a variety of (image, text) pairs.<br/>  
Object is Diffusion generative model sampling using CLIP.

![CLIP](https://github.com/Yeoneasy/clip_guided_diffusion/assets/129255517/0a8bed9a-00db-4185-b917-8c73367a5c54)

## Requirements

```
pip install -r requirements.txt
```

### Usage

Train and validaion

1. Download code zip
2. Download shapenet dataset [here](https://drive.google.com/drive/folders/1OkYgeRcIcLOFu1ft5mRODWNQaPJ0ps90) (cars_train, cars_val)
3. Put datasets seperately in directory (/db/cars_train, /db/cars_val)

## Running the tests

Explain how to run the automated tests for this system

### Break down into end to end tests

Explain what these tests test and why

```
Give an example
```

### And coding style tests

Explain what these tests test and why

```
Give an example
```
