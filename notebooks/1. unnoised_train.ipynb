{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a9767b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "import clip\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from itertools import repeat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388c7e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('./outputs'): # make output dir\n",
    "    os.makedirs('./outputs')\n",
    "\n",
    "# Load the model\n",
    "device = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False) #pretrained model\n",
    "\n",
    "# cast fp32 to use vit-b/32 model\n",
    "model = model.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629bafdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process image-text data\n",
    "class ImgTextDataset(Dataset):\n",
    "    def __init__(self, list_image_path, list_text):\n",
    "        self.image_path = list_image_path\n",
    "        self.title  = clip.tokenize(list_text) # tokenize everthing\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.title)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = preprocess(Image.open(self.image_path[idx])) # Image from PIL module\n",
    "        title = self.title[idx]\n",
    "        return image,title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac75e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read text file\n",
    "def read_file_lines(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = [line.strip() for line in file.readlines()]\n",
    "        return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdd73df",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = '../../Data/cars_train'\n",
    "# set train image path and text data\n",
    "entries = os.listdir(datapath)\n",
    "text_path = './db/cars.txt'\n",
    "\n",
    "load_text = read_file_lines(text_path)\n",
    "list_image_path = [] # 2151*250 = 537642\n",
    "list_text= [] # 2151*250 = 537642\n",
    "\n",
    "#shape_dict = {} # key : list_text, value : image_paths\n",
    "shape_dict_index = {} # key : list_text, value : image_paths(index)\n",
    "\n",
    "for count, entry in enumerate(entries):\n",
    "    image_dir = datapath + f'/{entry}/rgb/'\n",
    "    image_paths = [os.path.join(image_dir, file) for file in os.listdir(image_dir)]\n",
    "\n",
    "    list_image_path.extend(image_paths)\n",
    "    list_text.extend(repeat(load_text[count], len(image_paths)))\n",
    "\n",
    "    key = load_text[count]\n",
    "\n",
    "    # shape_dict 만들기\n",
    "    # key가 이미 존재하면 해당 key의 value(리스트)에 이미지 경로들을 추가\n",
    "    # key가 존재하지 않으면 새로운 key와 빈 리스트를 생성하고 이미지 경로들을 추가\n",
    "    #shape_dict.setdefault(key, []).extend(image_paths)\n",
    "\n",
    "    # shape_dict_index 만들기\n",
    "    # key가 이미 존재하면 해당 key의 value(리스트)에 이미지 인덱스들을 추가\n",
    "    # key가 존재하지 않으면 새로운 key와 빈 리스트를 생성하고 이미지 인덱스들을 추가\n",
    "    shape_dict_index.setdefault(key, []).extend(range(len(list_image_path) - len(image_paths), len(list_image_path)))\n",
    "    \n",
    "shape_dict_index_key = list(shape_dict_index.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19471316",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = '../../Data/cars_train_val'\n",
    "# image-text val_dataset\n",
    "val_entries = os.listdir(datapath)\n",
    "val_image_path =[] # 2150*10 = 21510\n",
    "val_text = [] # 2150*10 = 21510\n",
    "val_dict_indexs = {} # key : list_text, value : val_image_paths(index)\n",
    "    \n",
    "for count, entry in enumerate(val_entries) :\n",
    "    val_image_dir = datapath + f'/{entry}/rgb/'\n",
    "    val_image_paths = [os.path.join(val_image_dir, file) for file in os.listdir(val_image_dir)]\n",
    "        \n",
    "    val_image_path.extend(val_image_paths)\n",
    "    val_text.extend(repeat(load_text[count], len(val_image_paths)))\n",
    "\n",
    "    key = load_text[count]\n",
    "\n",
    "    val_dict_indexs.setdefault(key, []).extend(range(len(val_image_path) - len(val_image_paths), len(val_image_path)))\n",
    "    \n",
    "# value shuffled\n",
    "val_dict_index = {}\n",
    "for key, values in val_dict_indexs.items():\n",
    "    val_values = values.copy()  \n",
    "    random.shuffle(val_values)  \n",
    "    val_dict_index[key] = val_values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee31e2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyBatchSampler(torch.utils.data.Sampler):\n",
    "        \n",
    "    def __init__(\n",
    "        self,\n",
    "        text_superset,\n",
    "        text2image_indices_dict,\n",
    "        num_batches: int,\n",
    "        batch_size: int,\n",
    "    ):\n",
    "        super().__init__(None)\n",
    "        \"\"\"\n",
    "        Args\n",
    "            text_superset: ex) ['a red car', ..., 'a blue car']\n",
    "            text2image_indices_dict: dictionary (key: text, value: indices)\n",
    "                ex)\n",
    "                    key: text\n",
    "                    value: [0, 5, 80, 5000, 14068]\n",
    "                    즉, value는 2151*250 개의 image path들의 indices\n",
    "            num_batches: batch의 개수 제한\n",
    "            batch_size: batch 크기\n",
    "        \"\"\"\n",
    "        self.text_superset = text_superset\n",
    "        self.text2image_indices_dict = text2image_indices_dict\n",
    "        self.num_batches = num_batches\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __iter__(self):\n",
    "            \n",
    "        batch_counter = 0\n",
    "        while batch_counter < self.num_batches:\n",
    "            # ['a red car', ..., 'a blue car']로부터 subset 추출\n",
    "            # sub_text_indices = [ 3, 0, 5, 1, 4 ...]\n",
    "            sub_text_indices = torch.randperm(len(self.text_superset))[\n",
    "                : self.batch_size\n",
    "            ]\n",
    "            # text_subset에 text_superset 인덱스에 해당하는 텍스트들을 저장 \n",
    "            text_subset = [\n",
    "                self.text_superset[sub_text_indices[i]] for i in range(self.batch_size)\n",
    "            ]\n",
    "            # text_subset의 각 text에 해당하는 image_path들 중 random하게 1씩 추출\n",
    "            # 각 image_path는 전체 image_pathes의 어떤 index에 해당함\n",
    "            batch = []\n",
    "            for text in text_subset:\n",
    "                _image_indices = self.text2image_indices_dict[text]\n",
    "                batch.append(random.choice(_image_indices))\n",
    "            np.random.shuffle(batch)\n",
    "            yield batch\n",
    "            batch_counter += 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_batches * self.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4e3b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# val sampler\n",
    "class ValBatchSampler(torch.utils.data.Sampler):\n",
    "        \n",
    "    def __init__(\n",
    "        self,\n",
    "        text_superset,\n",
    "        text2image_indices_dict,\n",
    "        num_batches: int,\n",
    "        batch_size: int,\n",
    "    ):\n",
    "        super().__init__(None)\n",
    "        \"\"\"\n",
    "        Args\n",
    "            text_superset: ex) ['a red car', ..., 'a blue car']\n",
    "            text2image_indices_dict: dictionary (key: text, value: indices)\n",
    "                ex)\n",
    "                    key: text\n",
    "                    value: [0, 5, 80, 5000, 14068]\n",
    "                    즉, value는 2151*250 개의 image path들의 indices\n",
    "            num_batches: batch의 개수 제한\n",
    "            batch_size: batch 크기\n",
    "        \"\"\"\n",
    "        self.text_superset = text_superset\n",
    "        self.text2image_indices_dict = text2image_indices_dict\n",
    "        self.num_batches = num_batches\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __iter__(self):\n",
    "            \n",
    "        batch_counter = 0\n",
    "        while batch_counter < self.num_batches:\n",
    "            # ['a red car', ..., 'a blue car']로부터 subset 추출\n",
    "            # sub_text_indices = [ 0, 1, 2, 3, 4 ...]\n",
    "            sub_text_indices = torch.arange(self.batch_size)\n",
    "            # text_subset에 text_superset 인덱스에 해당하는 텍스트들을 저장 \n",
    "            text_subset = [self.text_superset[i] for i in sub_text_indices]\n",
    "            # text_subset의 각 text에 해당하는 image_path들 중 순차적으로 1씩 추출\n",
    "            # 각 image_path는 전체 image_pathes의 어떤 index에 해당함\n",
    "            batch = []\n",
    "            for text in text_subset:\n",
    "                _image_indices = self.text2image_indices_dict[text]\n",
    "                batch.append(_image_indices[batch_counter % len(_image_indices)])\n",
    "            yield batch\n",
    "            batch_counter += 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_batches * self.batch_size        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4f7e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set args\n",
    "kwargs = {\"num_workers\": 4, \"pin_memory\": True} # window = 0\n",
    "loss_img = nn.CrossEntropyLoss()\n",
    "loss_txt = nn.CrossEntropyLoss()\n",
    "\n",
    "num_epoch = 100\n",
    "train_batches = 10000 # if batch_size = 43, 12503\n",
    "val_batches = 400 # If batch_size = 43, 500\n",
    "num_batch_size = 43\n",
    "\n",
    "text_superset = shape_dict_index_key\n",
    "text2image_indices_dict = shape_dict_index\n",
    "\n",
    "# Early stopping\n",
    "patience = 4\n",
    "best_loss = float('inf')\n",
    "counter = 0\n",
    "\n",
    "# load train dataset\n",
    "train_dataset = ImgTextDataset(list_image_path, list_text)\n",
    "my_batch_sampler = MyBatchSampler(text_superset, text2image_indices_dict, train_batches, num_batch_size)\n",
    "train_dataloader = DataLoader(dataset=train_dataset, batch_sampler=my_batch_sampler, shuffle=False, **kwargs)\n",
    "    \n",
    "# load val dataset\n",
    "val_dataset = ImgTextDataset(val_image_path, val_text)\n",
    "val_batch_sampler = ValBatchSampler(text_superset, val_dict_index, val_batches, num_batch_size)\n",
    "val_dataloader = DataLoader(dataset=val_dataset, batch_sampler=val_batch_sampler, shuffle=False, **kwargs)    \n",
    "\n",
    "# AdamW optimizer\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-5,betas=(0.9,0.98),eps=1e-6, weight_decay=1e-5) #L2 \n",
    "epoch_losses = []\n",
    "val_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0851fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and validate process\n",
    "for epoch in range(num_epoch):\n",
    "        \n",
    "    model.train()\n",
    "    train_loss = 0.0 # initialize epoch loss\n",
    "\n",
    "    for batch in tqdm(train_dataloader,total=train_batches, desc=f'epoch : {epoch + 1}/{num_epoch}'):\n",
    "\n",
    "        # gradient zero\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # input values\n",
    "        images,texts = batch\n",
    "\n",
    "        images= images.to(device)\n",
    "        texts = texts.to(device)\n",
    "            \n",
    "        # expectation values\n",
    "        logits_per_image, logits_per_text = model(images, texts)\n",
    "\n",
    "        ground_truth = torch.arange(len(images),dtype=torch.long,device=device)\n",
    " \n",
    "        # back-propagation\n",
    "        total_loss = (loss_img(logits_per_image,ground_truth) + loss_txt(logits_per_text,ground_truth))/2\n",
    "        total_loss.backward()\n",
    "\n",
    "        # accumulate batch loss\n",
    "        train_loss += total_loss\n",
    "\n",
    "        # optimizer to next step\n",
    "        optimizer.step()\n",
    "\n",
    "    epoch_losses.append(train_loss.item() / len(train_dataloader))\n",
    "           \n",
    "    # Print the epoch loss\n",
    "    print(f'Epoch {epoch + 1}/{num_epoch}, Loss: {epoch_losses[-1]}')\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0.0 # initialize val loss\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_dataloader,total=val_batches, desc=f'val_epoch : {epoch + 1}/{num_epoch}'):\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            images,texts = batch\n",
    "\n",
    "            images= images.to(device)\n",
    "            texts = texts.to(device)\n",
    "                \n",
    "            logits_per_image, logits_per_text = model(images, texts)\n",
    "\n",
    "            ground_truth = torch.arange(len(images),dtype=torch.long,device=device)\n",
    "                \n",
    "            val_total_loss = (loss_img(logits_per_image,ground_truth) + loss_txt(logits_per_text,ground_truth))/2\n",
    "            val_loss += val_total_loss\n",
    "\n",
    "        val_losses.append(val_loss.item() / len(val_dataloader))\n",
    "            \n",
    "        # print the epoch loss\n",
    "        print(f'val_Epoch {epoch + 1}/{num_epoch}, val_Loss: {val_losses[-1]}')\n",
    "        \n",
    "    if val_loss < best_loss: # Early stopping check\n",
    "        best_loss = val_loss\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "\n",
    "    if counter >= patience:\n",
    "        print(f\"Early stopping after {epoch} epochs.\")\n",
    "        break\n",
    "    \n",
    "torch.save(model.state_dict(), os.path.join('./outputs', f'clip_trained.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b541d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the training loss over epochs\n",
    "plt.plot(epoch_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
