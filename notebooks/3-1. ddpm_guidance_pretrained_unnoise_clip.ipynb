{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# CLIP 관련 모듈\n",
    "import clip\n",
    "import torch\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
    "from torchvision.transforms import ToPILImage\n",
    "from PIL import Image\n",
    " \n",
    "# image 관련 모듈\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "\n",
    "# 작업 공간 설정\n",
    "WORKSPACE_DIR = '../'\n",
    "data_path = os.path.join(WORKSPACE_DIR, 'Data')\n",
    "\n",
    "# device 설정 (gpu index는 항상 동일하게 설정)\n",
    "device = \"cuda:1\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지를 시각화하는 함수\n",
    "def show_image(image):\n",
    "    image = image.cpu().numpy().transpose((1, 2, 0))\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# 이미지를 파일로 저장하는 함수\n",
    "def save_image(image, save_path):\n",
    "    image = image.cpu().numpy().transpose((1, 2, 0))\n",
    "    image = (image * 255).astype('uint8')  # 이미지를 0~255 범위로 변환\n",
    "    pil_image = Image.fromarray(image)\n",
    "    pil_image.save(save_path)\n",
    "\n",
    "# 이미지를 시각화하고 파일로 저장하는 함수 (p = 1이면 이미지를 출력함)\n",
    "def show_and_save_image(image, save_path, p = 0):\n",
    "    if p == 1:\n",
    "        show_image(image)\n",
    "    save_image(image, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dir(dirname):\n",
    "    try:\n",
    "        if not (os.path.isdir(dirname)):\n",
    "            os.makedirs(dirname)\n",
    "    except OSError:\n",
    "        print(f\"Failed Create Your Directory : {dirname}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from denoising_diffusion_pytorch import Unet\n",
    "from guided_diffusion import GaussianDiffusion # dedoising_diffusion_pytorch 내부의 sampling 함수는 샘플링시 guidance를 받지 못함\n",
    "from trainer import Trainer\n",
    "\n",
    "model = Unet(\n",
    "    dim = 64,\n",
    "    dim_mults = (1, 2, 4, 8)\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "diffusion = GaussianDiffusion(\n",
    "    model,\n",
    "    image_size = 32,\n",
    "    timesteps = 1000,           # number of steps\n",
    "    objective = 'pred_v',       # 학습을 'pred_v'로 했는지, 'pred_noise'로 했는지 확인하고 변경\n",
    "    sampling_timesteps =  1000   # timestep <= sampling_timesteps가 커야만 guidance를 적용해서 샘플링 가능\n",
    ")\n",
    "\n",
    "dataset_name = 'cars_shapenet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_path = os.path.join('../diffusion_model')\n",
    "print(res_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_dir(res_path)\n",
    "\n",
    "trainer = Trainer(\n",
    "    diffusion,\n",
    "    data_path + '/cars_train_test',   # 사용할 데이터 위치\n",
    "    dataset_name = dataset_name,\n",
    "    train_batch_size = 32,\n",
    "    train_lr = 8e-5,                  # 러닝 레이트\n",
    "    train_num_steps = 700000,         # 총 training steps\n",
    "    gradient_accumulate_every = 2,    # gradient accumulation steps\n",
    "    ema_decay = 0.995,                # exponential moving average decay\n",
    "    amp = True,                       # turn on mixed precision\n",
    "    calculate_fid = False,            # training 중 FID score 산출 여부\n",
    "    results_folder = res_path,\n",
    "    gpu_index = 1                     # 사용할 gpu 번호\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델을 로드하고 샘플링 결과를 보여줌 (내부적으로 모델을 호출하는 기능이 있음)\n",
    "trainer.sample_images_at_milestone(700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# CLIP 모델 초기화\n",
    "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "# vit-b/32 model을 fp32로 cast\n",
    "clip_model = clip_model.float()\n",
    "\n",
    "clip_model_name = \"pretrained\"\n",
    "\n",
    "# 저장된 state_dict 로드\n",
    "state_dict = torch.load(load_path, map_location=device)\n",
    "\n",
    "# If the saved model is a ScriptModule, unwrap it\n",
    "if isinstance(state_dict, torch.jit.ScriptModule):\n",
    "    state_dict = state_dict.state_dict()\n",
    "\n",
    "# 모델에 state_dict 적용\n",
    "clip_model.load_state_dict(state_dict, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_file_path = \"../db/cars_s.txt\" \n",
    "with open(class_file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    classes = [line.strip() for line in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 클래스를 모델에 전달하여 특징 벡터(임베딩) 계산\n",
    "class_embeddings = []\n",
    "for c in classes:\n",
    "    text = clip.tokenize([c]).to(device)\n",
    "    class_embedding = clip_model.encode_text(text)\n",
    "    class_embeddings.append(class_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_cond_fn(x, t, classifier, y, classifier_scale=1):\n",
    "    assert y is not None\n",
    "    with torch.enable_grad():\n",
    "        x_in = x.detach().requires_grad_(True)\n",
    "        x_in = diffusion.unnormalize(x_in)\n",
    "        x_in_upsample = torch.nn.functional.upsample(x_in, size=224, mode=\"bicubic\")\n",
    "        image_features = classifier(x_in_upsample)\n",
    "        logits = image_features @ y.t()\n",
    "        grad = torch.autograd.grad(logits.sum(), x_in)[0] * classifier_scale\n",
    "        return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier =  clip_model.visual  # CLIP 모델의 이미지 분류기 사용\n",
    "\n",
    "batch_size = 25\n",
    "idx = 0\n",
    "\n",
    "for c_embedding in class_embeddings:\n",
    "    sampled_images = diffusion.sample(\n",
    "        batch_size = batch_size,\n",
    "        cond_fn = classifier_cond_fn, \n",
    "        guidance_kwargs={\n",
    "            \"classifier\":classifier,\n",
    "            \"y\":c_embedding,\n",
    "            \"classifier_scale\": 0.1,\n",
    "        }\n",
    "    )\n",
    "    sampled_images.shape\n",
    "\n",
    "    print(classes[idx])\n",
    "    path_prefix = os.path.join('../unnoise/samples', clip_model_name, classes[idx])\n",
    "    make_dir(path_prefix)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        image_path = os.path.join(path_prefix, str(i).zfill(4) + \".png\")\n",
    "        show_and_save_image(sampled_images[i], image_path)\n",
    "\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
